{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Importing "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93a7c2027bde9d17"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:04.209996900Z",
     "start_time": "2023-11-17T17:37:57.957590600Z"
    }
   },
   "id": "1bddc529ad81424"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Reading, Exploration, and Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d00107ac1a776f9e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Loading and Initial Exploration\n",
    "Loading the dataset from the 'lateness_data.json' file and examining its initial structure."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99b2d5bef342fa8f"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "  direct_delivery batched_pickup transport_type          order_time  \\\n0             yes            yes     automobile 2023-10-09 19:23:55   \n1             yes            yes     automobile 2023-07-31 11:43:13   \n2             yes            yes        bicycle 2023-08-21 19:35:37   \n3             yes            yes     automobile 2023-09-06 00:19:29   \n4             yes            yes     automobile 2023-09-06 19:23:28   \n\n   delivery_distance  order_preparation_time  responsible_id  store_latitude  \\\n0               7798                      10            4444       55.795518   \n1                553                      10            3798       55.783786   \n2                711                      20            7595       55.729464   \n3               3538                      10            3797       55.731702   \n4               4169                      10            9509       55.781360   \n\n   store_longitude  client_latitude  client_longitude status  status_time  \n0        37.631224        55.780525         37.700847  early           18  \n1        37.624401        55.781943         37.628641  early            6  \n2        37.692976        55.732003         37.689528  early            9  \n3        37.581492        55.726069         37.604986  early           16  \n4        37.677339        55.787238         37.700311  early            5  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>direct_delivery</th>\n      <th>batched_pickup</th>\n      <th>transport_type</th>\n      <th>order_time</th>\n      <th>delivery_distance</th>\n      <th>order_preparation_time</th>\n      <th>responsible_id</th>\n      <th>store_latitude</th>\n      <th>store_longitude</th>\n      <th>client_latitude</th>\n      <th>client_longitude</th>\n      <th>status</th>\n      <th>status_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>yes</td>\n      <td>yes</td>\n      <td>automobile</td>\n      <td>2023-10-09 19:23:55</td>\n      <td>7798</td>\n      <td>10</td>\n      <td>4444</td>\n      <td>55.795518</td>\n      <td>37.631224</td>\n      <td>55.780525</td>\n      <td>37.700847</td>\n      <td>early</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>yes</td>\n      <td>yes</td>\n      <td>automobile</td>\n      <td>2023-07-31 11:43:13</td>\n      <td>553</td>\n      <td>10</td>\n      <td>3798</td>\n      <td>55.783786</td>\n      <td>37.624401</td>\n      <td>55.781943</td>\n      <td>37.628641</td>\n      <td>early</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>yes</td>\n      <td>yes</td>\n      <td>bicycle</td>\n      <td>2023-08-21 19:35:37</td>\n      <td>711</td>\n      <td>20</td>\n      <td>7595</td>\n      <td>55.729464</td>\n      <td>37.692976</td>\n      <td>55.732003</td>\n      <td>37.689528</td>\n      <td>early</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>yes</td>\n      <td>yes</td>\n      <td>automobile</td>\n      <td>2023-09-06 00:19:29</td>\n      <td>3538</td>\n      <td>10</td>\n      <td>3797</td>\n      <td>55.731702</td>\n      <td>37.581492</td>\n      <td>55.726069</td>\n      <td>37.604986</td>\n      <td>early</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>yes</td>\n      <td>yes</td>\n      <td>automobile</td>\n      <td>2023-09-06 19:23:28</td>\n      <td>4169</td>\n      <td>10</td>\n      <td>9509</td>\n      <td>55.781360</td>\n      <td>37.677339</td>\n      <td>55.787238</td>\n      <td>37.700311</td>\n      <td>early</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'lateness_data.json'\n",
    "data = pd.read_json(file_path)\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:05.553697100Z",
     "start_time": "2023-11-17T17:38:04.209996900Z"
    }
   },
   "id": "9635071bbfe7f47f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Target Variable Transformation\n",
    "Transforming the 'status' column to numerical values using label encoding.\n",
    "For our classification task, it's essential to convert categorical labels (early, late, on time) into a numerical format to train the model effectively."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ee4f7f01e111391"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "status_mapping = {\n",
    "    'early': 2,\n",
    "    'late': 0,\n",
    "    'on time': 1\n",
    "}\n",
    "\n",
    "data['status'] = data['status'].map(status_mapping)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:05.608438600Z",
     "start_time": "2023-11-17T17:38:05.553697100Z"
    }
   },
   "id": "c8dff1a1e1500483"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### One-Hot Encoding for Categorical Features\n",
    "Applying one-hot encoding to the 'direct_delivery' and 'batched_pickup' columns.\n",
    "One-hot encoding is used to handle categorical variables, creating binary columns for each category. This is crucial for feeding categorical data into machine learning models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fa3b9a440ae4f6"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['direct_delivery'], drop_first=True)\n",
    "data = pd.get_dummies(data, columns=['batched_pickup'], drop_first=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:05.868286400Z",
     "start_time": "2023-11-17T17:38:05.576364200Z"
    }
   },
   "id": "dbc3b43a4540046f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Encode Transport Type\n",
    "Create a mapping and use label encoding for the \"transport_type\" variable.\n",
    "Similar to the target variable, label encoding is applied to the \"transport_type\" feature to convert categorical data into a numerical format for model training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb0881a2e1bb8e26"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "transport_type_mapping = {\n",
    "    'foot': 0,\n",
    "    'scooter': 1,\n",
    "    'bicycle': 2,\n",
    "    'automobile': 3\n",
    "}\n",
    "data['transport_type'] = data['transport_type'].map(transport_type_mapping)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:05.868286400Z",
     "start_time": "2023-11-17T17:38:05.682602200Z"
    }
   },
   "id": "28878c3d172da7c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sort and Drop Time-related Columns\n",
    "Sort the dataset based on the \"order_time\" column and drop it.\n",
    "Sorting by time is essential in time-series data. Additionally, dropping the \"order_time\" column removes unnecessary temporal information for the current modeling task."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2c2e43647140d1c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data.sort_values(by='order_time', inplace=True)\n",
    "data.drop('order_time', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:05.868286400Z",
     "start_time": "2023-11-17T17:38:05.717032400Z"
    }
   },
   "id": "ecb2f70c3cdc5329"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prepare Features and Targets\n",
    "Create feature matrix X and target variables y for classification and regression tasks.\n",
    "This step involves splitting the dataset into features (X) and target variables (y) required for training the multi-task model. The \"status\" variable is used for classification, and the \"status_time\" variable is used for regression."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf3516ad53fd0fe3"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "X = data.drop(['status', 'status_time'], axis=1)\n",
    "y_classification = data[['status']]\n",
    "y_regression = data[['status_time']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:05.868286400Z",
     "start_time": "2023-11-17T17:38:05.769533300Z"
    }
   },
   "id": "a6ad564ccc20b095"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Split Data into Training and Validation Sets\n",
    "Perform a train-test split to create training and validation sets for features and target variables in both classification and regression tasks.\n",
    "\n",
    "This step is crucial for evaluating the model's performance on data it hasn't seen during training. It ensures that the model generalizes well to new data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "109e661c8f371b76"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X_train, X_val, y_classification_train, y_classification_val, y_regression_train, y_regression_val = train_test_split(\n",
    "    X, y_classification, y_regression, test_size=0.2, shuffle=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:05.868286400Z",
     "start_time": "2023-11-17T17:38:05.792803800Z"
    }
   },
   "id": "de5b067e83266005"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Scaling\n",
    "Scaling selected numerical features using StandardScaler.\n",
    "Scaling is necessary to bring numerical features to a similar scale, preventing certain features from dominating the learning process and ensuring effective model training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84e2e9e336bbb7f8"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "columns_to_scale = ['delivery_distance', 'order_preparation_time', 'responsible_id', 'store_latitude', 'store_longitude', 'client_latitude', 'client_longitude']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
    "X_val[columns_to_scale] = scaler.transform(X_val[columns_to_scale])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:05.978841500Z",
     "start_time": "2023-11-17T17:38:05.816710800Z"
    }
   },
   "id": "81bc82223b8dde6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Selection with Lasso Regression\n",
    "Apply Lasso regression for feature selection in both classification and regression tasks.\n",
    "Lasso regression helps identify and retain the most relevant features by penalizing the model for unnecessary ones. This step improves model interpretability and can enhance generalization.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc86b76d148716a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Classification Task"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5185b44e7e40801"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_train, y_classification_train)\n",
    "feature_importance_classification = lasso.coef_\n",
    "important_dict_classification = dict(zip(X_train.columns, feature_importance_classification))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:07.040472700Z",
     "start_time": "2023-11-17T17:38:05.881266400Z"
    }
   },
   "id": "9d1460ca910caeb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select features with weights less than 0.01"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76495c7bec8c6cfe"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "feature_drop_classification = [feature_drop for feature_drop in important_dict_classification if important_dict_classification[feature_drop] < 0.01]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:07.046763200Z",
     "start_time": "2023-11-17T17:38:07.043233200Z"
    }
   },
   "id": "235804997f815f6f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Regression Task"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5391d458b6af919d"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "lasso.fit(X_train, y_regression_train)\n",
    "feature_importance_regression = lasso.coef_\n",
    "important_dict_regression = dict(zip(X_train.columns, feature_importance_regression))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:07.136846800Z",
     "start_time": "2023-11-17T17:38:07.046763200Z"
    }
   },
   "id": "2e22f5c78c0ac38e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select features with weights less than 0.01"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3a2296adfed0e88"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "feature_drop_regression = [feature_drop for feature_drop in important_dict_regression if important_dict_regression[feature_drop] < 0.01]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:07.138792100Z",
     "start_time": "2023-11-17T17:38:07.138792100Z"
    }
   },
   "id": "1fb4135fbac29e4f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Identify features common to both tasks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23d86179520d397f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "feature_drop_both_task = [feature_drop for feature_drop in feature_drop_classification if feature_drop in feature_drop_regression]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:07.161960700Z",
     "start_time": "2023-11-17T17:38:07.138792100Z"
    }
   },
   "id": "dfbcfd140ef35364"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Drop selected features from the training and validation sets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e5dd25b3f50d27d"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "X_train.drop(feature_drop_both_task, axis=1, inplace=True)\n",
    "X_val.drop(feature_drop_both_task, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:07.161960700Z",
     "start_time": "2023-11-17T17:38:07.149607900Z"
    }
   },
   "id": "65fbb9cf8aba2073"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecb3f22ed6f61456"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (88610, 6)\n",
      "X_val.shape = (22153, 6)\n",
      "y_classification_train.shape = (88610, 1)\n",
      "y_classification_val.shape = (22153, 1)\n",
      "y_regression_train.shape = (88610, 1)\n",
      "y_regression_val.shape = (22153, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train.shape = {X_train.shape}\")\n",
    "print(f\"X_val.shape = {X_val.shape}\")\n",
    "print(f\"y_classification_train.shape = {y_classification_train.shape}\")\n",
    "print(f\"y_classification_val.shape = {y_classification_val.shape}\")\n",
    "print(f\"y_regression_train.shape = {y_regression_train.shape}\")\n",
    "print(f\"y_regression_val.shape = {y_regression_val.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:07.258844Z",
     "start_time": "2023-11-17T17:38:07.154594200Z"
    }
   },
   "id": "444594f6f140fb0b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multi-Task Model Definition and Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "503f86a3ca6ed9d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Multi-Task Model Definition\n",
    "Define a multi-task neural network model with a simple architecture.\n",
    "The model aims to perform both classification and regression tasks simultaneously. This architecture includes four hidden layers and two output layers for classification and regression, respectively."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69b6b072583dea3e"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "\n",
    "        # Define hidden layers\n",
    "        self.hidden_layer1 = nn.Linear(input_dim, 128)\n",
    "        self.hidden_layer2 = nn.Linear(128, 64)\n",
    "        self.hidden_layer3 = nn.Linear(64, 32)\n",
    "        self.hidden_layer4 = nn.Linear(32, 16)\n",
    "\n",
    "        # Output layers for classification and regression\n",
    "        self.classification_output = nn.Linear(16, 3)\n",
    "        self.regression_output = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply ReLU activation to hidden layers\n",
    "        x = torch.relu(self.hidden_layer1(x))\n",
    "        x = torch.relu(self.hidden_layer2(x))\n",
    "        x = torch.relu(self.hidden_layer3(x))\n",
    "        x = torch.relu(self.hidden_layer4(x))\n",
    "\n",
    "        # For classification task use softmax activation function\n",
    "        class_output = torch.softmax(self.classification_output(x), dim=1)\n",
    "\n",
    "        return class_output, self.regression_output(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:07.258844Z",
     "start_time": "2023-11-17T17:38:07.170318600Z"
    }
   },
   "id": "c00acc355527f805"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Transformation\n",
    "Transform the data into a numeric format if it is not already in numeric form.\n",
    "Machine learning models, especially deep learning models, require numeric input. This step ensures that the data is in the correct format for training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "865736ac4962ad7f"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Convert data to float32\n",
    "X_train_values = X_train.values.astype(np.float32)\n",
    "X_val_values = X_val.values.astype(np.float32)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_values, dtype=torch.float32)\n",
    "\n",
    "# Convert classification target variables to long tensors\n",
    "y_classification_train_tensor = torch.tensor(y_classification_train.values, dtype=torch.long).squeeze()\n",
    "y_classification_val_tensor = torch.tensor(y_classification_val.values, dtype=torch.long).squeeze()\n",
    "\n",
    "# Convert regression target variables to float32 tensors\n",
    "y_regression_train_tensor = torch.tensor(y_regression_train.values, dtype=torch.float32).squeeze()\n",
    "y_regression_val_tensor = torch.tensor(y_regression_val.values, dtype=torch.float32).squeeze()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:07.330589Z",
     "start_time": "2023-11-17T17:38:07.177534Z"
    }
   },
   "id": "7cec131f26ed2e98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model, Loss Function, and Optimizer\n",
    "Create the multi-task model and define the loss function and optimizer.\n",
    "\n",
    "- **Model:** The architecture is designed as a multi-task model with shared hidden layers for both classification and regression tasks. This allows the model to extract hierarchical features that are useful for both tasks.\n",
    "  \n",
    "- **Loss Functions:**\n",
    "  - **Classification Task:** CrossEntropyLoss is chosen for the classification task. This loss function is appropriate when dealing with multiple classes. The model predicts the probability distribution over the three classes (late, on time, early), and CrossEntropyLoss measures the difference between the predicted distribution and the true distribution.\n",
    "  \n",
    "  - **Regression Task:** L1Loss (Mean Absolute Error) is used for the regression task. L1Loss measures the absolute difference between the predicted and true values. It is suitable for tasks where we want the model to predict a numeric value (delivery time) with a level of tolerance.\n",
    "\n",
    "**Optimizer:**  \n",
    "Adam optimizer is selected for parameter updates during training. Adam is an adaptive learning rate optimization algorithm that combines the benefits of two other extensions of stochastic gradient descent, AdaGrad and RMSProp. It is well-suited for training deep neural networks and often converges faster than traditional stochastic gradient descent."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dce78e84cf317c8"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "model = MultiTaskModel(input_dim=X_train.shape[1])\n",
    "\n",
    "classification_criterion = nn.CrossEntropyLoss()  \n",
    "regression_criterion = nn.L1Loss()  \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:07.912311700Z",
     "start_time": "2023-11-17T17:38:07.265858800Z"
    }
   },
   "id": "47d4b257a7e92f2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Loading\n",
    "\n",
    "Load the data into PyTorch DataLoader for both training and validation sets.\n",
    "\n",
    "**DataLoader:** DataLoader provides features like batching, shuffling, and parallel data loading, enhancing the training speed and efficiency."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ae1817946f17f4b"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Create DataLoader for training set\n",
    "data_loader = DataLoader(utils.data.TensorDataset(X_train_tensor, y_classification_train_tensor, y_regression_train_tensor), batch_size=16, shuffle=False)\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "data_val_loader = DataLoader(utils.data.TensorDataset(X_val_tensor, y_classification_val_tensor, y_regression_val_tensor), batch_size=16, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:07.937048500Z",
     "start_time": "2023-11-17T17:38:07.912311700Z"
    }
   },
   "id": "fc33d2681a677fca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TensorBoard Setup\n",
    "Initialize TensorBoard for monitoring the model's performance during training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e63b0fec42c3923"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Set up TensorBoard for logging\n",
    "# log_dir = './logs_task_1'\n",
    "# writer = SummaryWriter(log_dir)\n",
    "\n",
    "writer = SummaryWriter(comment='task_1.1_Model_first')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:38:07.937048500Z",
     "start_time": "2023-11-17T17:38:07.920690500Z"
    }
   },
   "id": "7aa9a064fcd732a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model Training\n",
    "Train the multi-task model using the defined architecture, loss functions, and optimization technique."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90574b3a557c60d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model Training\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    all_labels_classification = []\n",
    "    all_predictions_classification = []\n",
    "    mae_regression_error = []\n",
    "    loss_train = []\n",
    "\n",
    "    print(f\"epoch: {epoch + 1}\")\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    for batch_idx, (X_train_tensor_batched, target_classification, target_regression) in enumerate(data_loader):\n",
    "        # Forward\n",
    "        classification_output, regression_output = model(X_train_tensor_batched)\n",
    "\n",
    "        # Calculate loss function for each task\n",
    "        classification_loss = classification_criterion(classification_output, target_classification)\n",
    "        regression_loss = regression_criterion(regression_output, target_regression)\n",
    "\n",
    "        # Total loss function - sum loss functions for each tasks\n",
    "        total_loss = classification_loss + regression_loss\n",
    "\n",
    "        loss_train.append(total_loss.item())\n",
    "\n",
    "        predicted_labels_train = torch.argmax(classification_output, dim=1)\n",
    "        all_labels_classification.extend(target_classification.cpu().numpy())\n",
    "        all_predictions_classification.extend(predicted_labels_train.cpu().numpy())\n",
    "\n",
    "        mae_regression_error.append(mean_absolute_error(target_regression, regression_output.detach().numpy()))\n",
    "\n",
    "        # Backward and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Log metrics to Tensorboard\n",
    "    writer.add_scalar('Train/Total Loss', sum(loss_train) / len(loss_train), epoch)\n",
    "    writer.add_scalar('Train/Classification Accuracy', accuracy_score(all_labels_classification, all_predictions_classification), epoch)\n",
    "    writer.add_scalar('Train/Regression MAE', sum(mae_regression_error) / len(mae_regression_error), epoch)\n",
    "    writer.add_scalar('Train/Classification F1', f1_score(all_labels_classification, all_predictions_classification, average='weighted'), epoch)\n",
    "\n",
    "    all_labels_classification = []\n",
    "    all_predictions_classification = []\n",
    "    mae_regression_error = []\n",
    "    loss_val = []\n",
    "\n",
    "    # val\n",
    "    model.eval()\n",
    "    for batch_idx, (X_val_tensor_batched, target_classification, target_regression) in enumerate(data_val_loader):\n",
    "        # Predict\n",
    "        classification_output, regression_output = model(X_val_tensor_batched)\n",
    "\n",
    "        # Calculate loss function for each task\n",
    "        classification_loss = classification_criterion(classification_output, target_classification)\n",
    "        regression_loss = regression_criterion(regression_output, target_regression)\n",
    "\n",
    "        # Total loss function - sum loss functions for each tasks\n",
    "        total_loss = classification_loss + regression_loss\n",
    "\n",
    "        loss_val.append(total_loss.item())\n",
    "\n",
    "        predicted_labels_train = torch.argmax(classification_output, dim=1)\n",
    "        all_labels_classification.extend(target_classification.cpu().numpy())\n",
    "        all_predictions_classification.extend(predicted_labels_train.detach().numpy())\n",
    "\n",
    "        mae_regression_error.append(mean_absolute_error(target_regression, regression_output.detach().numpy()))\n",
    "\n",
    "    # Log metrics to Tensorboard\n",
    "    writer.add_scalar('Val/Total Loss', sum(loss_train) / len(loss_train), epoch)\n",
    "    writer.add_scalar('Val/Classification Accuracy', accuracy_score(all_labels_classification, all_predictions_classification), epoch)\n",
    "    writer.add_scalar('Val/Regression MAE', sum(mae_regression_error) / len(mae_regression_error), epoch)\n",
    "    writer.add_scalar('Val/Classification F1', f1_score(all_labels_classification, all_predictions_classification, average='weighted'), epoch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34cae2c0215163d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bec7a1b7353b07"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), 'multi_task_model.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:46:54.274215100Z",
     "start_time": "2023-11-17T17:46:53.801791500Z"
    }
   },
   "id": "3ff4ce50dd4f3242"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load tensorboard"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "177cc72808c5a201"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-5b7df8917b046855\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-5b7df8917b046855\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:46:54.323576700Z",
     "start_time": "2023-11-17T17:46:53.824557500Z"
    }
   },
   "id": "b85d4361f5696b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Close Tensorboard writer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "474d7521fa9d97b"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:46:54.350275800Z",
     "start_time": "2023-11-17T17:46:53.941116500Z"
    }
   },
   "id": "67432107f2e79739"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Improved Multi-Task Model Definition, Training, and Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9484349223b6d06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Classification Head Model\n",
    "Define a separate model head for the classification task with a modified architecture.\n",
    "\n",
    "**Task-Specific Architecture:** The classification head has a distinct architecture tailored for classifying whether an order will be delivered early, on time, or late."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e2e2474876f01f1"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Classification Head Model\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "\n",
    "        # Define hidden layers\n",
    "        self.hidden_layer1 = nn.Linear(input_size, 128)\n",
    "        self.hidden_layer2 = nn.Linear(128, 64)\n",
    "        self.hidden_layer3 = nn.Linear(64, 32)\n",
    "        self.hidden_layer4 = nn.Linear(32, 16)\n",
    "\n",
    "        # Output layer for classification\n",
    "        self.classification_output = nn.Linear(16, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through hidden layers with ReLU activation\n",
    "        x = F.relu(self.hidden_layer1(x))\n",
    "        x = F.relu(self.hidden_layer2(x))\n",
    "        x = F.relu(self.hidden_layer3(x))\n",
    "        x = F.relu(self.hidden_layer4(x))\n",
    "\n",
    "        # Apply softmax activation for classification\n",
    "        x = F.softmax(self.classification_output(x), dim=1)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:46:54.350709Z",
     "start_time": "2023-11-17T17:46:53.965100200Z"
    }
   },
   "id": "9833ede683f869c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Regression Head Model\n",
    "\n",
    "Define a separate model head for the regression task with a modified architecture.\n",
    "\n",
    "**Task-Specific Architecture:** The regression head has a distinct architecture tailored for predicting the difference between expected delivery times."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84b1a084ce5ea5e9"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Regression Head Model\n",
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RegressionHead, self).__init__()\n",
    "\n",
    "        # Define hidden layers\n",
    "        self.hidden_layer1 = nn.Linear(input_size, 128)\n",
    "        self.hidden_layer2 = nn.Linear(128, 64)\n",
    "        self.hidden_layer3 = nn.Linear(64, 32)\n",
    "        self.hidden_layer4 = nn.Linear(32, 16)\n",
    "\n",
    "        # Output layer for regression\n",
    "        self.regression_output = nn.Linear(16, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through hidden layers with ReLU activation\n",
    "        x = torch.relu(self.hidden_layer1(x))\n",
    "        x = torch.relu(self.hidden_layer2(x))\n",
    "        x = torch.relu(self.hidden_layer3(x))\n",
    "        x = torch.relu(self.hidden_layer4(x))\n",
    "\n",
    "        # Linear output for regression\n",
    "        return self.regression_output(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:46:54.351281400Z",
     "start_time": "2023-11-17T17:46:53.992984300Z"
    }
   },
   "id": "3dfbc81a67aabd59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Improved Model Architecture and Optimizers\n",
    "Create separate model heads for classification and regression tasks with modified architectures. Also, define new loss functions and optimizers for each task.\n",
    "\n",
    "- **Task-Specific Models:** Two separate model heads are defined, each tailored to its specific task - classification and regression.\n",
    "- **Modified Architecture:** The architecture of the classification head includes a softmax activation function to produce probabilities for each class. The regression head, on the other hand, outputs a continuous value for the regression task.\n",
    "- **Custom Loss Functions:** Different loss functions are selected for each task. CrossEntropyLoss is suitable for classification, while Mean Squared Error (MSE) Loss is chosen for regression.\n",
    "- **Optimizers:** Separate optimizers are used for classification and regression tasks, allowing flexibility in adjusting learning rates independently."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98fc6bf5e100097a"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Classification Head Model\n",
    "model_classification = ClassificationHead(input_size=X_train.shape[1])\n",
    "\n",
    "# Regression Head Model\n",
    "model_regression = RegressionHead(input_size=7, hidden_size=5, output_size=1)\n",
    "\n",
    "# Loss functions for classification and regression\n",
    "classification_criterion_improved = nn.CrossEntropyLoss()\n",
    "regression_criterion_improved = nn.MSELoss()\n",
    "\n",
    "# Optimizers for classification and regression\n",
    "optimizer_classification_improved = optim.Adam(model_classification.parameters(), lr=0.001)\n",
    "optimizer_regression_improved = optim.Adam(model_regression.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:46:54.351281400Z",
     "start_time": "2023-11-17T17:46:54.015471100Z"
    }
   },
   "id": "699c60f7b5de896f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create DataLoaders for train and validation data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58b6f8a946a7ac9a"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "data_loader_improved = DataLoader(utils.data.TensorDataset(X_train_tensor, y_classification_train_tensor, y_regression_train_tensor), batch_size=16, shuffle=False) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:46:54.351281400Z",
     "start_time": "2023-11-17T17:46:54.133516400Z"
    }
   },
   "id": "5c64e2d27bac0140"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "data_val_loader_improved = DataLoader(utils.data.TensorDataset(X_val_tensor, y_classification_val_tensor, y_regression_val_tensor), batch_size=16, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:46:54.351281400Z",
     "start_time": "2023-11-17T17:46:54.133516400Z"
    }
   },
   "id": "38e0bb7c59673bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training Epoch\n",
    "Train the multi-task model for both classification and regression using an improved architecture. The training process involves forward passes for both tasks, calculation of loss functions, and optimization using task-specific optimizers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7942dc6171b28f4e"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def train_epoch(model_classification, model_regression, data_loader, optimizer_classification, optimizer_regression,\n",
    "                classification_criterion, regression_criterion, writer, epoch):\n",
    "    model_classification.train()\n",
    "    model_regression.train()\n",
    "\n",
    "    all_labels_classification_train = []\n",
    "    all_predictions_classification_train = []\n",
    "    mae_regression_error_train = []\n",
    "    loss_train = []\n",
    "\n",
    "    for batch_idx, (X_train_tensor_batched, target_classification, target_regression) in enumerate(data_loader):\n",
    "        # Forward pass for classification\n",
    "        classification_output_train = model_classification(X_train_tensor_batched)\n",
    "\n",
    "        # Calculate classification loss\n",
    "        classification_loss_train = classification_criterion(classification_output_train, target_classification)\n",
    "\n",
    "        optimizer_classification.zero_grad()\n",
    "        classification_loss_train.backward()\n",
    "        optimizer_classification.step()\n",
    "\n",
    "        # Detach intermediate value to avoid graph retention issues\n",
    "        detached_classification_output_train = torch.argmax(classification_output_train, dim=1).detach().unsqueeze(1)\n",
    "\n",
    "        # Forward pass for regression\n",
    "        # Combine X_train_tensor_batched and classification_output_train\n",
    "        combined_input_train = torch.cat((X_train_tensor_batched, detached_classification_output_train), dim=1)\n",
    "\n",
    "        regression_output_train = model_regression(combined_input_train)\n",
    "\n",
    "        # Calculate regression loss\n",
    "        regression_loss_train = regression_criterion(regression_output_train, target_regression)\n",
    "\n",
    "        optimizer_regression.zero_grad()\n",
    "        regression_loss_train.backward()\n",
    "        optimizer_regression.step()\n",
    "\n",
    "        # Metrics calculation for training\n",
    "        predicted_labels_train = torch.argmax(classification_output_train, dim=1)\n",
    "        all_labels_classification_train.extend(target_classification.cpu().numpy())\n",
    "        all_predictions_classification_train.extend(predicted_labels_train.cpu().numpy())\n",
    "\n",
    "        mae_error_train = torch.abs(regression_output_train - target_regression).mean().item()\n",
    "        mae_regression_error_train.append(mae_error_train)\n",
    "\n",
    "        loss_train.append(classification_loss_train.item() + regression_loss_train.item())\n",
    "\n",
    "    # Log metrics to Tensorboard for training\n",
    "    writer.add_scalar('Train Improved/Total Loss', sum(loss_train) / len(loss_train), epoch)\n",
    "    writer.add_scalar('Train Improved/Classification Accuracy', accuracy_score(all_labels_classification_train, all_predictions_classification_train), epoch)\n",
    "    writer.add_scalar('Train Improved/Regression MAE', sum(mae_regression_error_train) / len(mae_regression_error_train), epoch)\n",
    "    writer.add_scalar('Train Improved/Classification F1', f1_score(all_labels_classification_train, all_predictions_classification_train, average='weighted'), epoch)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:46:54.351281400Z",
     "start_time": "2023-11-17T17:46:54.133516400Z"
    }
   },
   "id": "8e70a034d1aa91b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluation Epoch Explanation\n",
    "Perform the evaluation of the multi-task model on the validation set for both classification and regression tasks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "beb295d638f589aa"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def eval_epoch(model_classification, model_regression, data_loader, classification_criterion, regression_criterion, writer, epoch):\n",
    "    # Set models to evaluation mode\n",
    "    model_classification.eval()\n",
    "    model_regression.eval()\n",
    "\n",
    "    # Initialize lists to store evaluation metrics\n",
    "    all_labels_classification_eval = []\n",
    "    all_predictions_classification_eval = []\n",
    "    mae_regression_error_eval = []\n",
    "    loss_eval = []\n",
    "\n",
    "    # Disable gradient computation during evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X_eval_tensor_batched, target_classification_eval, target_regression_eval) in enumerate(data_loader):\n",
    "            # Forward pass for classification\n",
    "            classification_output_eval = model_classification(X_eval_tensor_batched)\n",
    "\n",
    "            # Calculate classification loss\n",
    "            classification_loss_eval = classification_criterion(classification_output_eval, target_classification_eval)\n",
    "\n",
    "            # Detach intermediate value to avoid graph retention issues\n",
    "            detached_classification_output_eval = torch.argmax(classification_output_eval, dim=1).detach().unsqueeze(1)\n",
    "\n",
    "            # Forward pass for regression\n",
    "            # Combine X_eval_tensor_batched and classification_output_eval\n",
    "            combined_input_eval = torch.cat((X_eval_tensor_batched, detached_classification_output_eval), dim=1)\n",
    "\n",
    "            regression_output_eval = model_regression(combined_input_eval)\n",
    "\n",
    "            # Calculate regression loss\n",
    "            regression_loss_eval = regression_criterion(regression_output_eval, target_regression_eval)\n",
    "\n",
    "            # Metrics calculation for evaluation\n",
    "            predicted_labels_eval = torch.argmax(classification_output_eval, dim=1)\n",
    "            all_labels_classification_eval.extend(target_classification_eval.cpu().numpy())\n",
    "            all_predictions_classification_eval.extend(predicted_labels_eval.cpu().numpy())\n",
    "\n",
    "            mae_error_eval = torch.abs(regression_output_eval - target_regression_eval).mean().item()\n",
    "            mae_regression_error_eval.append(mae_error_eval)\n",
    "\n",
    "            loss_eval.append(classification_loss_eval.item() + regression_loss_eval.item())\n",
    "\n",
    "    # Metrics calculation for evaluation\n",
    "    eval_accuracy = accuracy_score(all_labels_classification_eval, all_predictions_classification_eval)\n",
    "    eval_mae = sum(mae_regression_error_eval) / len(mae_regression_error_eval)\n",
    "    eval_loss = sum(loss_eval) / len(loss_eval)\n",
    "\n",
    "    # Log metrics to Tensorboard for evaluation\n",
    "    writer.add_scalar('Eval Improved/Total Loss', eval_loss, epoch)\n",
    "    writer.add_scalar('Eval Improved/Classification Accuracy', eval_accuracy, epoch)\n",
    "    writer.add_scalar('Eval Improved/Regression MAE', eval_mae, epoch)\n",
    "    writer.add_scalar('Eval Improved/Classification F1', f1_score(all_labels_classification_eval, all_predictions_classification_eval, average='weighted'), epoch)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:46:54.352286800Z",
     "start_time": "2023-11-17T17:46:54.150199400Z"
    }
   },
   "id": "4aa8767801b8032a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TensorBoard Setup\n",
    "Initialize TensorBoard for monitoring the model's performance during training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63578e75a2c4fc92"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Set up TensorBoard for logging\n",
    "# log_dir = './logs_task_1'\n",
    "# writer = SummaryWriter(log_dir)\n",
    "\n",
    "writer = SummaryWriter(comment='task_1.2_Model_second')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:46:54.352286800Z",
     "start_time": "2023-11-17T17:46:54.168050300Z"
    }
   },
   "id": "e2501ca2945d4fb8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model Training and Evaluation\n",
    "Train and evaluate the improved multi-task model for a specified number of epochs.\n",
    "\n",
    "Training the model involves optimizing its parameters using the training dataset, while evaluation helps assess its performance on the validation set. Iterating through epochs allows the model to learn and adjust its weights over multiple passes through the training data, enhancing its ability to make accurate predictions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61e1228556e83306"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# Model Training and Evaluation\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"epoch = {epoch + 1}\")\n",
    "\n",
    "    # Training\n",
    "    train_epoch(model_classification, model_regression, data_loader_improved,\n",
    "                optimizer_classification_improved, optimizer_regression_improved,\n",
    "                classification_criterion_improved, regression_criterion_improved, writer, epoch)\n",
    "\n",
    "    # Evaluation\n",
    "    eval_epoch(model_classification, model_regression, data_val_loader_improved,\n",
    "               classification_criterion_improved, regression_criterion_improved, writer, epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:59:49.525477300Z",
     "start_time": "2023-11-17T17:46:54.212785800Z"
    }
   },
   "id": "d4dea42ba24ad7c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save trained model parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "866b473971bfd9e6"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "torch.save(model_classification.state_dict(), 'model_classification.pth')\n",
    "torch.save(model_regression.state_dict(), 'model_regression.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:59:49.536106700Z",
     "start_time": "2023-11-17T17:59:49.525477300Z"
    }
   },
   "id": "9c39b2b1d7186f61"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Reload tensorboard"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac9f9805d39aa205"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-476ceff6cc83b61a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-476ceff6cc83b61a\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:59:49.699383900Z",
     "start_time": "2023-11-17T17:59:49.545494500Z"
    }
   },
   "id": "9882a288f2d9c110"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Close writer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d92a76684f9d29fd"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T17:59:49.700641700Z",
     "start_time": "2023-11-17T17:59:49.565194600Z"
    }
   },
   "id": "6bfd7f13e6bbfc92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
